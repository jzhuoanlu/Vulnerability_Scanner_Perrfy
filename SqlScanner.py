import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

# initialize an HTTP session
session = requests.Session()
# set of internal links that will be crawled through
internal_urls = set()
# set of the information for each crawled url
url_info = []

def is_valid(url):
    """ Returns whether or not the inputed url is valid
    
    Parameters
    ----------
    url : str
        The url of the website being tested

    Returns
    -------
     : bool
        True if valid. Otherwise, false
    """
    # parse though the url and test for the protocol and network location
    parsed = urlparse(url)
    return bool(parsed.netloc) and bool(parsed.scheme)

def get_internal_urls(url):
    """ Returns the internal urls linked to the inputed url.
    
    Parameters
    ----------
    url : str
        The url of the website being scrapped

    Returns
    -------
    : set
        a set containing all the internal links on the page.
    """

    # this will be the returned set of internal urls found.
    # Note that it is a set because we don't want repeats
    urls = set()
    domain_name = urlparse(url).netloc

    # using bs, find all the a tags in the url
    soup = BeautifulSoup(requests.get(url).content, "html.parser")
    for a_tag in soup.findAll("a"):
        # get the href attribute
        href = a_tag.attrs.get("href")
        # continue if href is empty
        if href == "" or href is None:
            continue
        # make href an absolute url 
        href = urljoin(url, href)
        # parse through the href to remove parameters and recreate the correct url
        parsed_href = urlparse(href)
        href = parsed_href.scheme + "://" + parsed_href.netloc + parsed_href.path

        # continue if href isn't valid
        if not is_valid(href):
            continue

        # continue if href is already in the set
        if href in internal_urls:
            continue
        # only consider the internal urls
        if domain_name in href:
            urls.add(href)
            # this we will add to the global internal urls set, which is used to crawl the internal urls
            internal_urls.add(href)
    return urls
    
def crawl_scan(url, max_urls=30):
    """ Calls the crawl_helper method to crawl and scan through the url

    Parameters
    ----------
    url : str
        The url of the website being scrapped
    max_urls : int
        max number of urls that is crawled. Default is 30.

    Returns
    -------
    url_info : list
        returns the global var url_info that will be populated by crawl_helper with info about each crawled url.
    """

    crawl_helper(url, max_urls)
    return url_info

# counts the number of urls visited to make sure its less than max_urls
visited_counter = 0

def crawl_helper(url, max_urls=30):
    """ Crawls a url, extracts the urls, and scan for sql injections.
    
    Parameters
    ----------
    url : str
        The url of the website being scrapped
    max_urls : int
        max number of urls that is crawled. Default is 30.

    Returns
    -------
    """
    # use the counter to track the number of visited urls
    global visited_counter
    visited_counter += 1

    # scan that url for sql injection vulnerabilities and appends to url_info
    url_info.append(scan_sql_injection(url))
    print("Crawling " + url)

    # get the internal links on that url
    links = get_internal_urls(url)
    for link in links:
        if visited_counter > max_urls:
            break
        crawl_helper(link, max_urls=max_urls)

def get_all_forms(url):
    """ Returns a list of the form tags on the website
    
    Parameters
    ----------
    url : str
        The url of the website being scrapped

    Returns
    -------
    soup.find_all : list
        a list containing info on all form tags
    """

    # beautiful soup grabs the text of the HTML file and uses html.parser to 
    # parse through it for form tags
    soup = BeautifulSoup(session.get(url).content, "html.parser")
    return soup.find_all("form")


def get_form_attributes(form):
    """ Returns a dict of the information decoded from the form tag.

    Parameters
    ----------
    form : str
        The form tag to decode

    Returns
    -------
    attributes : dict
        a dict containing the attributes of the inputted form tag
    """

    attributes = {}

    # get the action attribute
    try:
        action = form.attrs.get("action").lower()
    except:
        action = None
    attributes["action"] = action  # save into details

    # get the method attribute
    try:
        method = form.attrs.get("method", "get").lower()
    except:
        method = None
    attributes["method"] = method  # save into details

    # get a list of the input attribute of the form tag
    inputs = []
    for input_tag in form.find_all("input"):
        type = input_tag.attrs.get("type", "text")
        name = input_tag.attrs.get("name")
        value = input_tag.attrs.get("value", "")
        inputs.append({"type": type, "name": name, "value": value})

    attributes["inputs"] = inputs  # save into details
    return attributes

def is_vul(response):
    """ Returns a boolean judging whether or not the response from contains an error 
    Note: the errors here are in no means exhaustive. These are just some simple ones.

    Parameters
    ----------
    response : 
        Response from the session making a get request

    Returns
    -------
     : bool
        a boolean signifying whether or not there is an error
    """
    errors = {
        # MySQL
        "you have an error in your sql syntax;",
        "warning: mysql",
        # SQL Server
        "unclosed quotation mark after the character string",
        # Oracle
        "quoted string not properly terminated",
    }
    for error in errors:
        if error in response.content.decode().lower():
            # return true if there is an error
            return True
    return False 


def scan_sql_injection(url):
    """ Returns a dictionary containing infomation sql vulnerability information about the url. 
    It tests both the url and by submiting a form on the webpage.

    Parameters
    ----------
    url : str 
        url of the webpage being tested.

    Returns
    -------
    info : dict
        dictionary containing information about if the webpage is vulnerable, and if it is, then its url and the form that is vulnerable.
    """

    # test for sql injection on the url
    # we test it by checking if there is an open quote at the end of the url
    info = {"Vulnerable": False, "URL": url, "Form": None}
    for char in "\"'":
        new_url = url + char
        # GET request
        response = session.get(new_url)
        if is_vul(response):
            # SQL Injection detected on the URL itself
            # return "Found Sql Injection Vulnerability in URL link:" + new_url
            info["Vulnerable"] = True
            info["URL"] = new_url
            return info 

    # grab all the form tags on the webpage
    forms = get_all_forms(url)

    # test by submiting a test form to each of the forms on the webpage
    for form in forms:
        # get the attributes of the form
        attributes = get_form_attributes(form)
        # now submit the form and test for vulnerabilities
        for char in "\"'":
            test_submit = {}
            for attr in attributes["inputs"]:
                # for hidden forms, try the default value with the char attached to it.
                if attr["type"] == "hidden" or attr["value"]:
                    try:
                        test_submit[attr["name"]] = attr["value"] + char
                    except:
                        pass
                # otherwise, try with "test" and a char attached.
                elif attr["type"] != "submit":
                    # all others except submit, use some junk data with special character
                    test_submit[attr["name"]] = f"test{char}"
            # join the url with the action 
            url = urljoin(url, attributes["action"])
            # response contains the HTTP response which will test for vulnerabilities
            if attributes["method"] == "post":
                response = session.post(url, data=test_submit)
            elif attributes["method"] == "get":
                response = session.get(url, params=test_submit)
            # test whether the resulting page is vulnerable
            if is_vul(response):
                #return "Found Sql Injection Vulnerability in URL link: " + url + "/n Form: " + attributes
                info["vul"] = True
                info["url"] = url
                info["Form"] = attributes
                return info
    return info